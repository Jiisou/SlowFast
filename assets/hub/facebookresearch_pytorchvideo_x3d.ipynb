{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c0d7fcea",
      "metadata": {
        "id": "c0d7fcea"
      },
      "source": [
        "# X3D\n",
        "\n",
        "*Author: FAIR PyTorchVideo*\n",
        "\n",
        "**X3D networks pretrained on the Kinetics 400 dataset**\n",
        "\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "#### install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/fvcore'\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y ffmpeg libsm6 libxext6\n",
        "\n",
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXMwJilZrqHW",
        "outputId": "73c8fc27-35a0-476a-a1b3-b108a9621174"
      },
      "id": "xXMwJilZrqHW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fvcore\n",
            "  Cloning https://github.com/facebookresearch/fvcore to /tmp/pip-req-build-d_58e5fa\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/fvcore /tmp/pip-req-build-d_58e5fa\n",
            "  Resolved https://github.com/facebookresearch/fvcore to commit 9d9285d9cc39723578b3423aa24552926590a0c3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore==0.1.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (3.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore==0.1.6) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore==0.1.6)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore==0.1.6) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore==0.1.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.6-py3-none-any.whl size=65628 sha256=641b95f4289b8c3631dcc9455718d2509220f26f09a89c12a09739b4b2e4fde2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-70yory8i/wheels/90/c0/1d/1429dd554467d53593be45b5b45a1ff39ea16299518145f638\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=83352d2302e80d3127a6cd14d00b26de9f4e8871981e29cfef73931ee38f7a4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.6 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,867 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,599 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,968 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Fetched 36.2 MB in 5s (6,737 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.3-1build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Collecting av\n",
            "  Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-16.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Imports\n",
        "\n",
        "Load the model:"
      ],
      "metadata": {
        "id": "DDM2cVoLr4O_"
      },
      "id": "DDM2cVoLr4O_"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c84b32ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c84b32ed",
        "outputId": "de0804d0-79a9-4037-817c-a817503cc80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/X3D_S.pyth\" to /root/.cache/torch/hub/checkpoints/X3D_S.pyth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.4M/29.4M [00:01<00:00, 22.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Choose the `x3d_s` model\n",
        "model_name = 'x3d_s'\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5151878c",
      "metadata": {
        "id": "5151878c"
      },
      "source": [
        "Import remaining functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5fc0302d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc0302d",
        "outputId": "9579cf1b-c040-4780-f29a-3441fbd3712f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/data/frame_video.py:106: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  return [int(c) if c.isdigit() else c for c in re.split(\"(\\d+)\", text)]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import urllib\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8593c1f6",
      "metadata": {
        "id": "8593c1f6"
      },
      "source": [
        "#### Setup\n",
        "\n",
        "Set the model to eval mode and move to desired device."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXypxC_NsEpP",
        "outputId": "5e280c9f-47b9-43e4-f4c2-3cd005ba7c31"
      },
      "id": "DXypxC_NsEpP",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a38cb73f",
      "metadata": {
        "id": "a38cb73f"
      },
      "outputs": [],
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cpu\"\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424b24da",
      "metadata": {
        "id": "424b24da"
      },
      "source": [
        "Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "563dc260",
      "metadata": {
        "id": "563dc260"
      },
      "outputs": [],
      "source": [
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e9961f72",
      "metadata": {
        "id": "e9961f72"
      },
      "outputs": [],
      "source": [
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28831bd",
      "metadata": {
        "id": "c28831bd"
      },
      "source": [
        "#### Define input transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d00f3a03",
      "metadata": {
        "id": "d00f3a03"
      },
      "outputs": [],
      "source": [
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "frames_per_second = 30\n",
        "model_transform_params  = {\n",
        "    \"x3d_xs\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 4,\n",
        "        \"sampling_rate\": 12,\n",
        "    },\n",
        "    \"x3d_s\": {\n",
        "        \"side_size\": 182,\n",
        "        \"crop_size\": 182,\n",
        "        \"num_frames\": 13,\n",
        "        \"sampling_rate\": 6,\n",
        "    },\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Get transform parameters based on model\n",
        "transform_params = model_transform_params[model_name]\n",
        "\n",
        "# Note that this transform is specific to the slow_R50 model.\n",
        "# 비디오 데이터를 X3D 같은 딥러닝 모델에 입력하기 전 데이터를 일정한 규격으로 맞추기 위한 전처리\n",
        "transform =  ApplyTransformToKey( # pytorchvideo.transforms.ApplyTransformToKey\n",
        "    key=\"video\",\n",
        "    transform=Compose( # torchvision.transforms.Compose\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),  # pytorchvideo.transforms.UniformTemporalSubsample\n",
        "            Lambda(lambda x: x/255.0), # torchvision.transforms.Lambda\n",
        "            NormalizeVideo(mean, std), # torchvision.transforms._transforms_video.NormalizeVideo\n",
        "            ShortSideScale(size=transform_params[\"side_size\"]), #pytorchvideo.transforms.ShortSideScale\n",
        "            CenterCropVideo(            # torchvision.transforms._transforms_video\n",
        "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (transform_params[\"num_frames\"] * transform_params[\"sampling_rate\"])/frames_per_second"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ecb5434",
      "metadata": {
        "id": "9ecb5434"
      },
      "source": [
        "#### Run Inference\n",
        "\n",
        "Download an example video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d7a22cf8",
      "metadata": {
        "id": "d7a22cf8"
      },
      "outputs": [],
      "source": [
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f456ad9e",
      "metadata": {
        "id": "f456ad9e"
      },
      "source": [
        "Load the video and transform it to the input format required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3a17edf8",
      "metadata": {
        "id": "3a17edf8"
      },
      "outputs": [],
      "source": [
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "video = EncodedVideo.from_path(video_path) # pytorchvideo.data.encoded_video.EncodedVideo\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = inputs.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VV7fhb56DRd",
        "outputId": "11038794-77f3-4507-e57f-5abf8236c898"
      },
      "id": "3VV7fhb56DRd",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytorchvideo.data.encoded_video_pyav.EncodedVideoPyAV at 0x7a16e0454c20>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7910399",
      "metadata": {
        "id": "e7910399"
      },
      "source": [
        "#### Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.size()) # c, t, h, W\n",
        "print(inputs[None, ...].size()) # 1, C, T, H, W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uQy5Sds8Jwp",
        "outputId": "993633f5-33e0-4654-9882-11f597301884"
      },
      "id": "7uQy5Sds8Jwp",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 13, 182, 182])\n",
            "torch.Size([1, 3, 13, 182, 182])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1529bab5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1529bab5",
        "outputId": "01059745-aa8b-4d3e-cb7d-1115acaa30fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 predicted labels: archery, throwing axe, golf driving, golf chipping, opening bottle\n"
          ]
        }
      ],
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs[None, ...])\n",
        "\n",
        "# Get the predicted classes\n",
        "# 로짓 값을 클래스명으로 매핑시키기 위한 후처리.\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names)) # 양궁, 도끼 던지기, 골프 높게 치는 샷, 골프 낮게 치는 샷, 보틀 열기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds.size() # 400 = Kinetics 데이터셋의 클래스 개수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGxGmV2c6zWw",
        "outputId": "7aeeab97-1adc-4161-93c7-a6c9cb2a527a"
      },
      "id": "UGxGmV2c6zWw",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 400])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3a6d61",
      "metadata": {
        "id": "9b3a6d61"
      },
      "source": [
        "### Model Description\n",
        "X3D model architectures are based on [1] pretrained on the Kinetics dataset.\n",
        "\n",
        "| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
        "| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |\n",
        "| X3D      | XS    | 4x12                       | 69.12 | 88.63 | 0.91      | 3.79     |\n",
        "| X3D      | S     | 13x6                       | 73.33 | 91.27 | 2.96      | 3.79     |\n",
        "| X3D      | M     | 16x5                       | 75.94 | 92.72 | 6.72      | 3.79     |\n",
        "\n",
        "\n",
        "### References\n",
        "[1] Christoph Feichtenhofer, \"X3D: Expanding Architectures for\n",
        "    Efficient Video Recognition.\" https://arxiv.org/abs/2004.04730"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list |grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY_sVaYX_0pA",
        "outputId": "c8d3676b-56c9-4272-e231-5f846df7fe73"
      },
      "id": "bY_sVaYX_0pA",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                                    2.9.0+cpu\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.9.0+cpu\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.24.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list |grep fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkc8gTA8_12y",
        "outputId": "b8c33b7a-283a-4589-97f3-8d63fbd911cf"
      },
      "id": "bkc8gTA8_12y",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fvcore                                   0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc --version"
      ],
      "metadata": {
        "id": "mM3OFvktDroI",
        "outputId": "13d8a324-afef-40ec-93ac-7a054644a442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mM3OFvktDroI",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "pfH-t1k8DsoA",
        "outputId": "41538aaf-fafb-4f7a-ca20-c1187f55dc33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pfH-t1k8DsoA",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list |grep pytorchvideo"
      ],
      "metadata": {
        "id": "amgK7JmID5II"
      },
      "id": "amgK7JmID5II",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directly Inference on UCF-CRIME"
      ],
      "metadata": {
        "id": "wai3gA_kExi_"
      },
      "id": "wai3gA_kExi_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"clip duration: {clip_duration} (sec)\")\n",
        "\n",
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "ucf_video_path=\"/content/Abuse001_x264_7-12.mp4\"\n",
        "ucf_video = EncodedVideo.from_path(ucf_video_path) # pytorchvideo.data.encoded_video.EncodedVideo\n",
        "\n",
        "# Load the desired clip\n",
        "ucf_video_data = ucf_video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "ucf_video_transformed = transform(ucf_video_data)\n",
        "ucf_inputs = ucf_video_transformed[\"video\"] # 인코딩된 비디오 텐서값만\n",
        "ucf_inputs = ucf_inputs.to(device)\n",
        "\n",
        "ucf_preds = model(ucf_inputs[None, ...]) # 배치차원 추가하여 (1, C, T, H, W)\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "ucf_pred_classes = post_act(ucf_preds)\n",
        "ucf_pred_top5 = ucf_pred_classes.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "ucf_pred_top5_names = [kinetics_id_to_classname[int(i)] for i in ucf_pred_top5]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(ucf_pred_top5_names))"
      ],
      "metadata": {
        "id": "tPmgl2GMEZJQ",
        "outputId": "2b4666e0-4a6f-46dd-ba35-e928c1ba983a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tPmgl2GMEZJQ",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clip duration: 2.6 (sec)\n",
            "Top 5 predicted labels: moving furniture, cleaning floor, hoverboarding, using remote controller (not gaming), garbage collecting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ucf_video_path=\"/content/Abuse001_x264_7-12.mp4\"\n",
        "ucf_video = EncodedVideo.from_path(ucf_video_path) # pytorchvideo.data.encoded_video.EncodedVideo\n",
        "\n",
        "# Load the desired clip\n",
        "ucf_video_data = ucf_video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "ucf_video_transformed = transform(ucf_video_data)\n",
        "ucf_inputs = ucf_video_transformed[\"video\"] # 인코딩된 비디오 텐서값만\n",
        "ucf_inputs = ucf_inputs.to(device)\n",
        "\n",
        "ucf_preds = model(ucf_inputs[None, ...]) # 배치차원 추가하여 (1, C, T, H, W)\n",
        "ucf_pred_classes = post_act(ucf_preds)   # post_act = torch.nn.Softmax(dim=1)\n",
        "\n",
        "# 확률값(v)과 인덱스(i) 모두 가져오기.\n",
        "top5_probs, top5_indices = ucf_pred_classes.topk(k=5)\n",
        "\n",
        "# 텐서 형태를 출력을 위한 넘파이로 변환\n",
        "probs = top5_probs[0].detach().cpu().numpy()\n",
        "indices = top5_indices[0].detach().cpu().numpy()\n",
        "\n",
        "# 클래스 이름과 확률을 매핑해 출력.\n",
        "print(\"Top 5 Predictions:\")\n",
        "for i in range(5):\n",
        "    class_name = kinetics_id_to_classname[int(indices[i])]\n",
        "    score = probs[i] * 100  # 퍼센트(%) 단위로 변환\n",
        "    print(f\"{i+1}: {class_name:<20} | Score: {score:.2f} (%)\")"
      ],
      "metadata": {
        "id": "Owj39BVUFYyB",
        "outputId": "218c9ab4-f98f-4dce-b3a5-3e9e01abac7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Owj39BVUFYyB",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Predictions:\n",
            "1: moving furniture     | Score: 15.94 (%)\n",
            "2: cleaning floor       | Score: 15.64 (%)\n",
            "3: hoverboarding        | Score: 10.06 (%)\n",
            "4: using remote controller (not gaming) | Score: 7.65 (%)\n",
            "5: garbage collecting   | Score: 6.81 (%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9JljRyyGxzB"
      },
      "id": "a9JljRyyGxzB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}